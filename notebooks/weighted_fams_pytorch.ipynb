{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f3fa64",
   "metadata": {},
   "source": [
    "# Entendiendo WeightedFamiliesLayerPytorch\n",
    "\n",
    "Este notebook explica paso a paso cómo funciona la clase `WeightedFamiliesLayerPytorch` con valores determinísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01c793b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446307f",
   "metadata": {},
   "source": [
    "## 1. La clase completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31674be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFamiliesLayerPytorch(nn.Module):\n",
    "    def __init__(self, num_models, num_classes, hidden_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # First layer: per-model, per-family\n",
    "        self.weights1 = nn.Parameter(tr.rand(num_models, num_classes, hidden_size))\n",
    "        self.bias1 = nn.Parameter(tr.zeros(num_classes, hidden_size))\n",
    "\n",
    "        # Second layer: per-family\n",
    "        self.weights2 = nn.Parameter(tr.rand(num_classes, hidden_size))\n",
    "        self.bias2 = nn.Parameter(tr.zeros(num_classes))\n",
    "\n",
    "    def forward(self, x): # x: (num_models, batch, num_classes)\n",
    "        # Expand x to (num_models, batch, num_classes, 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "\n",
    "        # First aggregation (models → hidden)\n",
    "        w1 = self.weights1.unsqueeze(1)  # (num_models, 1, num_classes, hidden_size)\n",
    "        b1 = self.bias1.unsqueeze(0) # (1, num_classes, hidden_size)\n",
    "        h = tr.sum(x * w1, dim=0) + b1\n",
    "        h = tr.relu(h)  # (batch, num_classes, hidden_size)\n",
    "\n",
    "        # Second aggregation (hidden → output)\n",
    "        out = tr.sum(h * self.weights2.unsqueeze(0), dim=-1) + self.bias2\n",
    "        return out  # (batch, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0eed35",
   "metadata": {},
   "source": [
    "## 2. Configuración de ejemplo\n",
    "\n",
    "Usaremos valores pequeños y determinísticos para entender cada paso:\n",
    "- **num_models = 2** (dos modelos en el ensemble)\n",
    "- **num_classes = 3** (tres familias de proteínas)\n",
    "- **batch = 2** (dos ejemplos en el batch)\n",
    "- **hidden_size = 2** (dimensión oculta pequeña)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d61eb139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros de la capa:\n",
      "weights1 shape: torch.Size([2, 3, 2])\n",
      "bias1 shape: torch.Size([3, 2])\n",
      "weights2 shape: torch.Size([3, 2])\n",
      "bias2 shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "num_models = 2\n",
    "num_classes = 3\n",
    "batch_size = 2\n",
    "hidden_size = 2\n",
    "\n",
    "# Crear la capa\n",
    "layer = WeightedFamiliesLayerPytorch(num_models, num_classes, hidden_size)\n",
    "\n",
    "print(\"Parámetros de la capa:\")\n",
    "print(f\"weights1 shape: {layer.weights1.shape}\")\n",
    "print(f\"bias1 shape: {layer.bias1.shape}\")\n",
    "print(f\"weights2 shape: {layer.weights2.shape}\")\n",
    "print(f\"bias2 shape: {layer.bias2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1b274c",
   "metadata": {},
   "source": [
    "## 3. Setear valores determinísticos\n",
    "\n",
    "Para entender mejor, asignamos valores fijos en lugar de aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba805fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos seteados exitosamente\n"
     ]
    }
   ],
   "source": [
    "# weights1: (num_models=2, num_classes=3, hidden_size=2)\n",
    "layer.weights1.data = tr.tensor([\n",
    "    [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]],  # Modelo 1\n",
    "    [[0.5, 1.0], [1.5, 2.0], [2.5, 3.0]]   # Modelo 2\n",
    "], dtype=tr.float32)\n",
    "\n",
    "# bias1: (num_classes=3, hidden_size=2)\n",
    "layer.bias1.data = tr.tensor([\n",
    "    [0.1, 0.2],  # Clase 0\n",
    "    [0.3, 0.4],  # Clase 1\n",
    "    [0.5, 0.6]   # Clase 2\n",
    "], dtype=tr.float32)\n",
    "\n",
    "# weights2: (num_classes=3, hidden_size=2)\n",
    "layer.weights2.data = tr.tensor([\n",
    "    [1.0, 1.0],  # Clase 0\n",
    "    [2.0, 2.0],  # Clase 1\n",
    "    [3.0, 3.0]   # Clase 2\n",
    "], dtype=tr.float32)\n",
    "\n",
    "# bias2: (num_classes=3)\n",
    "layer.bias2.data = tr.tensor([0.1, 0.2, 0.3], dtype=tr.float32)\n",
    "\n",
    "print(\"Pesos seteados exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7b834",
   "metadata": {},
   "source": [
    "## 4. Input de ejemplo\n",
    "\n",
    "Creamos un input `x` con forma `(num_models, batch, num_classes)` con valores determinísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "889dc48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      "tensor([[[0.1000, 0.2000, 0.3000],\n",
      "         [0.4000, 0.5000, 0.6000]],\n",
      "\n",
      "        [[0.7000, 0.8000, 0.9000],\n",
      "         [1.0000, 1.1000, 1.2000]]])\n",
      "Shape: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# x: (num_models=2, batch=2, num_classes=3)\n",
    "x = tr.tensor([\n",
    "    [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],  # Modelo 1, batch 0 y 1\n",
    "    [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]   # Modelo 2, batch 0 y 1\n",
    "], dtype=tr.float32)\n",
    "\n",
    "print(\"Input x:\")\n",
    "print(x)\n",
    "print(f\"Shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc0400",
   "metadata": {},
   "source": [
    "## 5. Paso 1: unsqueeze(-1)\n",
    "\n",
    "Añadimos una dimensión extra al final para poder multiplicar con los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d905aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x después de unsqueeze(-1):\n",
      "tensor([[[[0.1000],\n",
      "          [0.2000],\n",
      "          [0.3000]],\n",
      "\n",
      "         [[0.4000],\n",
      "          [0.5000],\n",
      "          [0.6000]]],\n",
      "\n",
      "\n",
      "        [[[0.7000],\n",
      "          [0.8000],\n",
      "          [0.9000]],\n",
      "\n",
      "         [[1.0000],\n",
      "          [1.1000],\n",
      "          [1.2000]]]])\n",
      "Shape: torch.Size([2, 2, 3, 1])\n",
      "\n",
      "Cambió de torch.Size([2, 2, 3]) a torch.Size([2, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x_expanded = x.unsqueeze(-1)\n",
    "print(\"x después de unsqueeze(-1):\")\n",
    "print(x_expanded)\n",
    "print(f\"Shape: {x_expanded.shape}\")\n",
    "print(f\"\\nCambió de {x.shape} a {x_expanded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da87b47",
   "metadata": {},
   "source": [
    "## 6. Paso 2: Preparar weights1 para broadcasting\n",
    "\n",
    "Expandimos `weights1` para que tenga una dimensión de batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2a1d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights1 original shape: torch.Size([2, 3, 2])\n",
      "w1 después de unsqueeze(1):\n",
      "tensor([[[[0.4602, 0.4135],\n",
      "          [0.8105, 0.1076],\n",
      "          [0.2113, 0.5382]]],\n",
      "\n",
      "\n",
      "        [[[0.3064, 0.7573],\n",
      "          [0.3055, 0.7222],\n",
      "          [0.1363, 0.6831]]]], grad_fn=<UnsqueezeBackward0>)\n",
      "Shape: torch.Size([2, 1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "w1 = layer.weights1.unsqueeze(1)\n",
    "print(\"weights1 original shape:\", layer.weights1.shape)\n",
    "print(\"w1 después de unsqueeze(1):\")\n",
    "print(w1)\n",
    "print(f\"Shape: {w1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d590e",
   "metadata": {},
   "source": [
    "## 7. Paso 3: Multiplicación $x * w1$ (primera agregación)\n",
    "\n",
    "Broadcasting permite multiplicar:\n",
    "- `x_expanded`: `(2, 2, 3, 1)`\n",
    "- `w1`: `(2, 1, 3, 2)`\n",
    "\n",
    "Resultado: `(2, 2, 3, 2)` (se expanden las dimensiones con 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce313d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_expanded * w1:\n",
      "tensor([[[[0.0460, 0.0414],\n",
      "          [0.1621, 0.0215],\n",
      "          [0.0634, 0.1615]],\n",
      "\n",
      "         [[0.1841, 0.1654],\n",
      "          [0.4053, 0.0538],\n",
      "          [0.1268, 0.3229]]],\n",
      "\n",
      "\n",
      "        [[[0.2145, 0.5301],\n",
      "          [0.2444, 0.5778],\n",
      "          [0.1226, 0.6148]],\n",
      "\n",
      "         [[0.3064, 0.7573],\n",
      "          [0.3361, 0.7944],\n",
      "          [0.1635, 0.8197]]]], grad_fn=<MulBackward0>)\n",
      "Shape: torch.Size([2, 2, 3, 2])\n",
      "\n",
      "Ejemplo para modelo=0, batch=0, clase=0:\n",
      "x[0,0,0] = 0.10\n",
      "w1[0,0,0,:] = tensor([0.4602, 0.4135], grad_fn=<SliceBackward0>)\n",
      "Resultado: tensor([0.0460, 0.0414], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "multiplied = x_expanded * w1\n",
    "print(\"x_expanded * w1:\")\n",
    "print(multiplied)\n",
    "print(f\"Shape: {multiplied.shape}\")\n",
    "\n",
    "print(\"\\nEjemplo para modelo=0, batch=0, clase=0:\")\n",
    "print(f\"x[0,0,0] = {x[0,0,0]:.2f}\")\n",
    "print(f\"w1[0,0,0,:] = {w1[0,0,0,:]}\")\n",
    "print(f\"Resultado: {multiplied[0,0,0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992cd21",
   "metadata": {},
   "source": [
    "## 8. Paso 4: Sumar sobre dim=0 (modelos)\n",
    "\n",
    "Colapsamos la dimensión de modelos, combinando las predicciones de ambos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d750a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suma sobre dim=0 (modelos):\n",
      "tensor([[[0.4500, 0.9000],\n",
      "         [1.8000, 2.4000],\n",
      "         [3.7500, 4.5000]],\n",
      "\n",
      "        [[0.9000, 1.8000],\n",
      "         [3.1500, 4.2000],\n",
      "         [6.0000, 7.2000]]], grad_fn=<SumBackward1>)\n",
      "Shape: torch.Size([2, 3, 2])\n",
      "\n",
      "Verificación manual para batch=0, clase=0:\n",
      "Modelo1: x[0,0,0] * w1[0,0,0,:] = 0.10 * tensor([1., 2.], grad_fn=<SliceBackward0>) = tensor([0.1000, 0.2000], grad_fn=<MulBackward0>)\n",
      "Modelo2: x[1,0,0] * w1[1,0,0,:] = 0.70 * tensor([0.5000, 1.0000], grad_fn=<SliceBackward0>) = tensor([0.3500, 0.7000], grad_fn=<MulBackward0>)\n",
      "Suma: tensor([0.4500, 0.9000], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "summed = tr.sum(multiplied, dim=0)\n",
    "print(\"Suma sobre dim=0 (modelos):\")\n",
    "print(summed)\n",
    "print(f\"Shape: {summed.shape}\")\n",
    "\n",
    "print(\"\\nVerificación manual para batch=0, clase=0:\")\n",
    "print(f\"Modelo1: x[0,0,0] * w1[0,0,0,:] = {x[0,0,0]:.2f} * {w1[0,0,0,:]} = {x[0,0,0] * w1[0,0,0,:]}\")\n",
    "print(f\"Modelo2: x[1,0,0] * w1[1,0,0,:] = {x[1,0,0]:.2f} * {w1[1,0,0,:]} = {x[1,0,0] * w1[1,0,0,:]}\")\n",
    "print(f\"Suma: {summed[0,0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d32be",
   "metadata": {},
   "source": [
    "## 9. Paso 5: Añadir bias1\n",
    "\n",
    "El bias tiene forma `(num_classes, hidden_size)`, lo expandimos para batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664147e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias1 original: torch.Size([3, 2])\n",
      "b1 después de unsqueeze(0): torch.Size([1, 3, 2])\n",
      "tensor([[[0.1000, 0.2000],\n",
      "         [0.3000, 0.4000],\n",
      "         [0.5000, 0.6000]]], grad_fn=<UnsqueezeBackward0>)\n",
      "\n",
      "Suma + bias1:\n",
      "tensor([[[0.5500, 1.1000],\n",
      "         [2.1000, 2.8000],\n",
      "         [4.2500, 5.1000]],\n",
      "\n",
      "        [[1.0000, 2.0000],\n",
      "         [3.4500, 4.6000],\n",
      "         [6.5000, 7.8000]]], grad_fn=<AddBackward0>)\n",
      "Shape: torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "b1 = layer.bias1.unsqueeze(0)\n",
    "print(\"bias1 original:\", layer.bias1.shape)\n",
    "print(\"b1 después de unsqueeze(0):\", b1.shape)\n",
    "print(b1)\n",
    "\n",
    "h_before_relu = summed + b1\n",
    "print(\"\\nSuma + bias1:\")\n",
    "print(h_before_relu)\n",
    "print(f\"Shape: {h_before_relu.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbbf41",
   "metadata": {},
   "source": [
    "## 10. Paso 6: Aplicar ReLU\n",
    "\n",
    "La activación ReLU convierte valores negativos a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0021d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de ReLU:\n",
      "tensor([[[0.5500, 1.1000],\n",
      "         [2.1000, 2.8000],\n",
      "         [4.2500, 5.1000]],\n",
      "\n",
      "        [[1.0000, 2.0000],\n",
      "         [3.4500, 4.6000],\n",
      "         [6.5000, 7.8000]]], grad_fn=<ReluBackward0>)\n",
      "Shape: torch.Size([2, 3, 2])\n",
      "\n",
      "Nota: ReLU(x) = max(0, x), así que los valores negativos se vuelven 0\n"
     ]
    }
   ],
   "source": [
    "h = tr.relu(h_before_relu)\n",
    "print(\"Después de ReLU:\")\n",
    "print(h)\n",
    "print(f\"Shape: {h.shape}\")\n",
    "\n",
    "print(\"\\nNota: ReLU(x) = max(0, x), así que los valores negativos se vuelven 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f7510",
   "metadata": {},
   "source": [
    "## 11. Paso 7: Segunda agregación (hidden → output)\n",
    "\n",
    "Multiplicamos `h` por `weights2` y sumamos sobre la dimensión oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6994ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights2 original: torch.Size([3, 2])\n",
      "w2 después de unsqueeze(0): torch.Size([1, 3, 2])\n",
      "tensor([[[1., 1.],\n",
      "         [2., 2.],\n",
      "         [3., 3.]]], grad_fn=<UnsqueezeBackward0>)\n",
      "\n",
      "h * w2:\n",
      "tensor([[[ 0.5500,  1.1000],\n",
      "         [ 4.2000,  5.6000],\n",
      "         [12.7500, 15.3000]],\n",
      "\n",
      "        [[ 1.0000,  2.0000],\n",
      "         [ 6.9000,  9.2000],\n",
      "         [19.5000, 23.4000]]], grad_fn=<MulBackward0>)\n",
      "Shape: torch.Size([2, 3, 2])\n",
      "\n",
      "Suma sobre dim=-1 (hidden_size):\n",
      "tensor([[ 1.6500,  9.8000, 28.0500],\n",
      "        [ 3.0000, 16.1000, 42.9000]], grad_fn=<SumBackward1>)\n",
      "Shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "w2 = layer.weights2.unsqueeze(0)\n",
    "print(\"weights2 original:\", layer.weights2.shape)\n",
    "print(\"w2 después de unsqueeze(0):\", w2.shape)\n",
    "print(w2)\n",
    "\n",
    "# Multiplicación\n",
    "h_w2 = h * w2\n",
    "print(\"\\nh * w2:\")\n",
    "print(h_w2)\n",
    "print(f\"Shape: {h_w2.shape}\")\n",
    "\n",
    "# Suma sobre hidden_size (dim=-1)\n",
    "out_before_bias = tr.sum(h_w2, dim=-1)\n",
    "print(\"\\nSuma sobre dim=-1 (hidden_size):\")\n",
    "print(out_before_bias)\n",
    "print(f\"Shape: {out_before_bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9744a",
   "metadata": {},
   "source": [
    "## 12. Paso 8: Añadir bias2 (output final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02225106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output final:\n",
      "tensor([[ 1.7500, 10.0000, 28.3500],\n",
      "        [ 3.1000, 16.3000, 43.2000]], grad_fn=<AddBackward0>)\n",
      "Shape: torch.Size([2, 3])\n",
      "\n",
      "bias2: Parameter containing:\n",
      "tensor([0.1000, 0.2000, 0.3000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "out = out_before_bias + layer.bias2\n",
    "print(\"Output final:\")\n",
    "print(out)\n",
    "print(f\"Shape: {out.shape}\")\n",
    "\n",
    "print(\"\\nbias2:\", layer.bias2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15615401",
   "metadata": {},
   "source": [
    "## 13. Verificación: Comparar con la capa completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3d104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output de la capa completa:\n",
      "tensor([[ 1.7500, 10.0000, 28.3500],\n",
      "        [ 3.1000, 16.3000, 43.2000]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Output calculado manualmente:\n",
      "tensor([[ 1.7500, 10.0000, 28.3500],\n",
      "        [ 3.1000, 16.3000, 43.2000]], grad_fn=<AddBackward0>)\n",
      "\n",
      "¿Son iguales?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Calcular con la capa completa\n",
    "output_layer = layer(x)\n",
    "\n",
    "print(\"Output de la capa completa:\")\n",
    "print(output_layer)\n",
    "\n",
    "print(\"\\nOutput calculado manualmente:\")\n",
    "print(out)\n",
    "\n",
    "print(\"\\n¿Son iguales?\")\n",
    "print(tr.allclose(output_layer, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d810dd2",
   "metadata": {},
   "source": [
    "## 14. Resumen visual\n",
    "\n",
    "```\n",
    "Input: (num_models, batch, num_classes)\n",
    "   ↓ unsqueeze(-1)\n",
    "(num_models, batch, num_classes, 1)\n",
    "   ↓ × weights1 (broadcasted)\n",
    "(num_models, batch, num_classes, hidden_size)\n",
    "   ↓ sum(dim=0) + bias1\n",
    "(batch, num_classes, hidden_size)\n",
    "   ↓ ReLU\n",
    "(batch, num_classes, hidden_size)\n",
    "   ↓ × weights2 + sum(dim=-1) + bias2\n",
    "(batch, num_classes) ← Output final\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bcedd1",
   "metadata": {},
   "source": [
    "## 15. ¿Qué hace esta capa?\n",
    "\n",
    "1. **Primera capa**: Aprende una combinación ponderada de las predicciones de cada modelo para cada familia, proyectándolas a un espacio oculto (hidden_size). Esto permite capturar interacciones complejas.\n",
    "\n",
    "2. **ReLU**: Introduce no-linealidad, permitiendo que la red aprenda patrones más complejos que una simple suma ponderada.\n",
    "\n",
    "3. **Segunda capa**: Colapsa el espacio oculto de vuelta a una predicción por familia.\n",
    "\n",
    "**Diferencia con WeightedFamiliesLayer**: Esta versión tiene una capa oculta que permite aprender relaciones no-lineales entre los modelos, mientras que la original solo hace una suma ponderada lineal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb2pfam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
